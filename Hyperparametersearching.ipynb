{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to determine the best learning rate for each of the architecture after Making sure that trainable parameters are the same scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First section: Checking number of parameters and updating them until they are in the same scale of magnitute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import normflows as nf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader as dl\n",
    "from torch.utils.data import TensorDataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "base = nf.distributions.DiagGaussian(2)\n",
    "Models=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RotationalQuadraticSpline():\n",
    "    K = 7\n",
    "\n",
    "    latent_size = 2\n",
    "    hidden_units = 270\n",
    "    hidden_layers = 2\n",
    "\n",
    "    flows = []\n",
    "    for i in range(K):\n",
    "        flows += [nf.flows.AutoregressiveRationalQuadraticSpline(latent_size, hidden_layers, hidden_units)]\n",
    "        flows += [nf.flows.LULinearPermute(latent_size)]\n",
    "\n",
    "    # Set base distribution\n",
    "    q0 = base\n",
    "        \n",
    "    # Construct flow model\n",
    "    model = nf.NormalizingFlow(q0, flows)\n",
    "\n",
    "    return(model)\n",
    "\n",
    "Models.append(RotationalQuadraticSpline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real NVP Model here\n",
    "def Real_nvp_init():\n",
    "    num_layers = 160\n",
    "    Real_NVP_flows = []\n",
    "    for i in range(num_layers):\n",
    "        parameters = nf.nets.MLP([1,128,32,84,64,16,2],init_zeros=True)\n",
    "        Real_NVP_flows.append(nf.flows.AffineCouplingBlock(param_map=parameters))\n",
    "        Real_NVP_flows.append(nf.flows.Permute(2,mode=\"swap\"))\n",
    "\n",
    "    Real_NVP_model = nf.NormalizingFlow(q0=base,flows=Real_NVP_flows)\n",
    "\n",
    "    return Real_NVP_model\n",
    "\n",
    "Models.append(Real_nvp_init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAF_init():\n",
    "    MAF_flows = []\n",
    "    hidden = 256 #number of hidden units\n",
    "    \n",
    "    latent_size = 2 #input dimensions\n",
    "    \n",
    "    K=8\n",
    "\n",
    "    for i in range(K):\n",
    "        MAF_flows += [nf.flows.MaskedAffineAutoregressive(latent_size, hidden)]\n",
    "        MAF_flows += [nf.flows.Permute(2,mode=\"swap\")]\n",
    "\n",
    "    MAF_model = nf.NormalizingFlow(q0=base,flows=MAF_flows)\n",
    "\n",
    "    return MAF_model\n",
    "\n",
    "Models.append(MAF_init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Spline Flow\n",
    "def Neural_Spline_Flow():\n",
    "    K = 8\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    latent_size = 2\n",
    "    hidden_units = 252\n",
    "    hidden_layers = 2\n",
    "\n",
    "    neural_flows = []\n",
    "    for i in range(K):\n",
    "        neural_flows += [nf.flows.AutoregressiveRationalQuadraticSpline(latent_size, hidden_layers, hidden_units)]\n",
    "        neural_flows += [nf.flows.LULinearPermute(latent_size)]\n",
    "\n",
    "\n",
    "    Neural_Spline_model= nf.NormalizingFlow(q0=base, flows=neural_flows)\n",
    "\n",
    "    return Neural_Spline_model\n",
    "\n",
    "Models.append(Neural_Spline_Flow())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 2141738 trainable parameters\n",
      "model has 2187204 trainable parameters\n",
      "model has 2119716 trainable parameters\n",
      "model has 2139396 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "for model in Models:\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "    print(f'model has {params} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Section: Training the Models and seeing how they perform. Will come then adapt this section to create a \n",
    "funtion that will facilitate hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, all the models have similar number of trainable parameters. These are the models that I will use to compare and find their optimum learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, target_data_loader,learning_rate):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    tolerance = 0.0001\n",
    "    previous_loss = float('inf')\n",
    "    early_stop = False\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        if not early_stop:\n",
    "            model.train()\n",
    "            for data in target_data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                data = data[0].to(device)  # Extract data from TensorDataset and move to device\n",
    "                loss = model.forward_kld(data)\n",
    "                \n",
    "                if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                current_loss = loss.item()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    loss_diff = abs(current_loss - previous_loss)\n",
    "                    if loss_diff < tolerance:\n",
    "                        early_stop = True\n",
    "                        print(f\"Early stopping at epoch {epoch+1} with loss difference {loss_diff:.6f}\")\n",
    "                        break\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                previous_loss = current_loss\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
