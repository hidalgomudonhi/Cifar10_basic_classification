This project aims to increase the accuracy of an MLP using data augmentation and dropout without complicating the network.
It contains two pretrained models which show the progress, one has dropout only, the other is the raw file as optained from pytorch.
The third  is the CIFAR 10 which has the results of a 20% increase.

The last file is a polynomial one, aimed at showing how basic MLPs work
